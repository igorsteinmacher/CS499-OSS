# Paper Report

## Title
Attention Is All You Need

## Venue (journal/conference)
NeurIPS 2017 (Advances in Neural Information Processing Systems 30)

## Number of pages
11 pages

## Link to the paper online
https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf

## 2â€“3 sentence summary
This paper introduces the Transformer, a sequence-to-sequence model that replaces recurrence with self-attention. By using multi-head attention plus positional encodings, it trains efficiently and achieves strong machine translation results.
